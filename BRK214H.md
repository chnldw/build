Microsoft Build: Getting started with generative AI using Azure OpenAI Service
Dom Divakaruni, Pablo Castro, Jamie Ovenden, Charlotte Wood, Tim Gordon
May 2023

[MUSIC] 

DOM DIVAKARUNI: Well, hello everyone. Great to see you. My name is Dom Divakaruni, I lead Product for Azure OpenAI, and I'm joined by my esteemed colleague, Pablo Castro, who leads Azure Cognitive Search. I'm super happy to be here with you all here to talk about Azure OpenAI and the tools that we're building to help you develop applications with it, to develop Copilots with it, and excited to share a bunch of new announcements, some of which you've heard during Scott's talk, and dive into it in some more details here.

You've heard a lot about this, but again, setting the stage, the field is as old as some of us here in the room and it's really been accelerating since 2012, started with the AlexNet Paper and then the transformers in 2017 but really it's picked up steam in the last couple of years. None of us have seen this pace of innovation and it's really changing all our lives in such a meaningful way very quickly. Seen in no other than ChatGPT in how quickly it really captured all of our attention. In no time in history has technology or really any innovation captured all our interests so quickly and has reached the adoption levels of ChatGPT as we've seen. In really two months, it has scaled to 100 million users, and it just speaks to how useful and practical it is in all our daily lives and intuitive.

We at Azure AI are happy to build all these innovations into products to make them second nature for us to consume in our productivity suites, in search engines that we use as well as low-code, no-code tools that we all use in PowerBI and PowerApps. All that can zoom the powerful AI models in the customizable AI model, including Azure OpenAI, which is our service that brings together OpenAI's most recent innovation to life. You can build applications with it because that's what's most important. All of this runs on Azure Machine Learning, our massive-scale AI platform that helps all these innovations come to life.

We look at Azure OpenAI. What is it? It is our platform that helps us bring OpenAI's research to life. It rides on a foundation of enterprise security with all the highest security standards and what you expect from the cloud and what you can depend on, where your data is private and it remains in your tenant, and none of it is used to train or improve our systems or train machine learning models, and provides you with all of OpenAI's models as quickly as we possibly can, including the most recent GPT-4 model. It has really captured all the attention of the industry. It's lighting up all these unique use cases that haven't been possible before: summarization, reasoning over data, writing tools, co-generation, and now ushering in the era of Copilots.

GPT-4 is the latest to the state of the art from OpenAI and you've heard a lot about it. It achieves human-level perf on text generation. It lets you steer it with nuanced instructions. Actually, OpenAI will tell you that they've had this model done training and still in development for about six months. That time was carefully spent tuning the model to make it more steerable and useful when you instruct it to do something. It really shows.

I don't know if you all have had a chance to play with GPT4 either in ChatGPT or the Azure OpenAI Service, but what you're able to do with it is really remarkable. There was a time in the past where prompt engineering really was considered an art and it really took a lot of coaxing to get the model to do what you need it to do. Not anymore with GPT4. Just with a few instructions, what you can create with it is fricking phenomenal. All of that is really being used by thousands of customers. I think Scott cited 4,500 customers that are using Azure OpenAI today and really building with it. Coursera is a great example of a customer that is building AI-powered learning experience on our platform to help learners with an interactive learning experience that helps learning be a more personalized journey than one that is generic and crafted, and that's out there and we have to adapt to it.

As we look at this, we see three use cases really emerging. One is where AI is embedded into products we use like search and makes things fundamentally better. Search synthesis as opposed to search results on a page that you have to discern what the right thing is for you, and pretty simple use cases like generating blogs or helping us write emails. The next level above that is what we've seen just over the past few months with ChatGPT, where it's helping novices learn topics that they are not familiar with with a simple QA-based style learning. That really opens the doors for us to become more effective at everything we do by just simply asking the model to help be our Copilot to guide us along a task.

The last part where we're seeing these models really help and emerges to help experts be more efficient at what they do, offloading their tasks and helping the knowledge worker be more effective at what they do. We're seeing that in no other place. Epic Healthcare plays a major role in the healthcare industry, and we're helping them incorporate GPT-4 into electronic health records. They are helping medical professionals use their time more effectively. Another great example of helping the expert is DataRobot. DataRobot, our partner, incorporated Azure OpenAI to help their customer cohort, their customer base of data scientists and ML practitioners be more effective. Machine learning's been out there for a long time. Lots of customers use it in their mission critical applications as part of their workflow and the tasks that they do. What this integration helps customers at DataRobot do is it helps them do their jobs more effectively, it provides data scientists with some analysis which they can go fact check and helps accelerate their journey, helps create more models and put AI into more places.

With this, I'm happy to share a couple of announcements. We talked about the models that are available on the platform. This conference at Build is all about us taking that a step forward, helping you build Copilots along with these technologies. I'm happy to announce four different capabilities. Pablo will talk about one more with Azure Cognitive Search. The first two really help you extend the capability of the models, incorporate your own data, because that's really when it's useful is when you are able to apply your data to it and garner insights from it and help leverage the model to power insights and power your applications. Then the last two which I'll go into are innovations that will really provide you more control over the way you integrate the service into your applications and also a provision throughput, which is a capability for large-scale production applications. Let's dive into it.

To go from a 10,000 foot level down much lower, let's talk about the API interface that makes this possible today. It's called a chat completions API, which is all encompassing. In the past, you may have been familiar with the completions API, and used the completions API to ask the model, and it's a single question response interface. This one not only does everything the previous API did, but also enables chat, which enables these Copilot-like scenarios. Where, if you look at the code there, it allows for a couple different fields. One is the system field, which is really the system message or, as Kevin talked about, the meta prompt, which you can tailor to tell the model exactly how to behave, what to say, what not to say, what parameters to stay between and really steer the model for your particular application, for your particular use case. Then you can stick any number of examples into the prompt as long as the prompt length goes. Under the role of User and Assistant, you can provide the model with some examples to help steer it. In fact, what we're finding internally is that this paradigm of using the system message and examples in User and Assistant, gets you really a long way than the traditional approaches with using fine tuning where you really have to go low level. You have to train these models with your data. It's a tough mental model and paradigm for us to break where a lot of us in the machine learning field, fine tuning is just a natural thing. You have a base model, you've got to fine-tune it for your particular task, and what we're really seeing is that increasingly, with each generation of model, you're having to do that much less and less where the system message combined with examples that you can put in the prompt, really get you quite a distance.

Per all the keynotes and the talk so far, the challenge that we run into is great, the model is fantastic, it has a base level of knowledge, probably trained in 2022 with GPT-4. But it doesn't it doesn't have my context. I want to put my context into the prompt. The prompt length isn't long enough. What do I do? I don't want to have to keep sending the model all these tokens for my entire document at once. The conventional solution that we keep telling customers is build a vector database and at run-time, go find the relevant snippets that pertain to your query, craft your prompt, and run it. Has worked for a lot of customers for quite a while now, but we felt that this was an opportunity to make it easier for customers, and so I'd like to introduce Azure OpenAI on your data.

We've built a really easy way to make it easy for developers to ground Azure OpenAI on your data. You can combine a model like chatGPT, which is GPT 3.5 Turbo or GPT-4 with your own data and build your assistant that reasons over your private data, and it does it securely within your Azure tenant, in your Azure resource, no data is shared with anybody, your data is not used. Again, I'm trying to hammer this point home because it comes from a lot of customers. Your data is not used to improve our systems or train machine learning models, it's all in your own tenant.

We have a number of data sources to start; there, in the top blue, we'll be expanding to a number of additional data sources in the weeks and months to come. Really it is to extend the capability of these models for you to ground it on your own data. Why don't I flip to a demo of what this looks like? You've seen some of this onstage, but I can go into it in a little bit more depth. We all know how live demos go. Keeping my fingers crossed for the demo gods to shine, to smile on us.

Here I am in the Azure OpenAI Studio. This is being called the Azure AI Studio as we bring together a lot of industry open source models for you to really take advantage of the most cutting edge in AI. I'm going to flip to the ChatGPT playground. God, I hope this loads. It did. What I'm going to show you first is the base model without grounding with your data. I'm going to type in something simple. Think of this as a chat bot facing your employees for them to learn about some specifics in your health insurance plan. I'm going to look up on this. The model is either going to say I don't know or come up with an answer that's pretty generic. The model, sure enough, said it doesn't have information about specific Premera health plans.

Now I want to ground this with my data. We've introduced this new add your data field. By the way, I meant to say, before I get to add your data, on the system message, in case you're not familiar with it, this is where you can go craft your specific instructions to the model to tailor it to what you want to do. I'm going to go back to add your data. I'm going to add a data source. Here we have a number of options for you to use. Blob stores, you can upload files, whether that's PDFs, or Word documents, or what have you, and you can use your search index as your cognitive search, which I'm going to use for this demo. I'm going to choose a cognitive search service and I acknowledge that I may incur charges for the use of this.

This is going exceedingly slow. That doesn't seem to want to work. Let me try reloading the page. Pardon me folks, we'll get there. For instances like these, I do have a spare recording, so we'll have to go to that and you'll have to pretend like I'm doing this live. We'll just have to roll with it. You can see I recorded this this morning. I'm certainly not, it's great that AR/VR is not my day job. I clearly have this thing that it says it's recorded.

I'm going to jump in here and fast forward, I go add your data. At this point, I look through my data sources, I look at my cognitive search index, and choose. This is where I was before. I chose my search index. Now I want to map the columns in my index to citations that I want to see when it generates an answer. Perfect, I choose fields that match what I want, match my search index. This is key. I want to be able to say limit responses to what's in the data content. Oftentimes, you build this thing. You don't want the model to stray off in some random build, and provide your employees with something it's not supposed to respond to. You can certainly do that in the system prompter, you can craft it to say, don't do this. But you can also say limit responses to your data content. That's just it. Thank you. It's still top left. The winning answer goes to swap displays. Got it, awesome. It's teamwork, folks, teamwork makes dream work. We're here.

Now, I'm going to go ask it the same question, are eye exams covered under the Premera plan? It says yes. Eye exams are indeed covered and provide you a citation. Always check AI's work and you're able to go see on the right-hand side what document it'd pull up, did it do the right thing. Perfect. Now there's another neat feature that's baked into here, which is, I can go publish this as a Azure App Services web app, which then lets you easily provide an interface that folks can interact with. You can either test this out or you can go take that web app and extend it to your own use case. You can add your own authentication on it and make this your own for your own companies. This is the view of that web app. Imagine that I published it and you go ask, this is like a standard user interface. No one's actually going to go to the Azure OpenAI Studio. They're going to go to this interface that you built for your own company. This is another way to make it really easy for you to go build your own Copilot and present it out to your audience. Could you flip back to the slides for me, please? Awesome.

With that, I'm going to introduce you to Pablo, who will walk you under the covers of how you create your search index and what power is the experience that you just saw. Thank you, Pablo. 

PABLO CASTRO: Thanks, Dom. Hi, everyone. Good morning. As you probably already heard this morning during keynotes and whatnot, integrating large language models with your own data is one of the key challenges that we all face when constructing new generative AI applications. The emerging pattern for this is what we call Retrieval Augmented Generation. The idea of Retrieval Augmented Generation, or RAG, is to separate the large language model from an externalized knowledge base, all coordinated with an orchestrator that mediates interaction between these two components and the rest of the application experience.

There are different ways to bundle this together. Sometimes you'll build your own applications, maybe you're building your own Copilot or your own custom experience where you control the application user interface, you build your own orchestrator either with the technology Dom just showed, or maybe you're using long-chain or Semantic Kernel to orchestrate all of these things together. In the end, at that point you built the entire stack on your own and maybe you use a large language model and a retriever, or sometimes you'll build a plugin, which is a way to use extensibility in other systems like ChatGPT, or any of the other surface areas as we discussed this morning that now support plugins.

Now a key element is that in all of these scenarios, what we have is an externalized knowledge base. This allows us to bring data that is not about the model. We use the model because of its capabilities to absorb information, reason about it, and give us answers, but we don't want its knowledge. We want the knowledge to be the one we bring in through the knowledge base. If you think about the actual knowledge base, let's think about what is the problem statement we need to tackle. What we want to do is we want to externalize knowledge into this retriever. What the retrieval needs to do is find the most relevant pieces of information across a very large set, given just an unstructured query. The systems that do these have a name already, and they're called search engines. In the context of Azure, Azure Cognitive Search is a solution for retrieval that can power all of these applications. It'll bring all the building blocks that you'll need from data ingestion, to scaling, to enterprise-grade security, to dealing with written languages and whatnot.

Using Cognitive Search, you can now create a knowledge base there and along with a large language model, create an integrated experience for customers. Traditional methods for search are very often very effective for this, so you'll use things like keyword search, symbolic manipulation of text for linguistics and things like that. That has proven effective in a number of ways. That said, another emerging trend these days is to also use representation based retrieval or vector embeddings for retrieval, which are complimentary to traditional methods in the sense that they can do semantic base retrieval that's very useful in some cases.

Let me very quickly first establish what do we mean by semantic similarity. The idea is, you can learn a model such that imagine you have a high dimensional vector space that you train such that things that mean similar things are close together in that vector space. There are many ways to do this. I'm happy to talk about the details later, but think about that as the outcome. Now if you have a system like that, then you can encode maybe words or sentences or pictures or pieces of audio and as soon as they are all in this vector space, then you can define similarity as, hey, if I find vectors that are close together, then I found the things that are similar. You can do this using the Azure OpenAI embeddings API. There are many excellent open source embedding models like Sentence-BERT or CLIP, or you can build your own.

Now, once you have an embedding, the problem of vector retrieval kind of split in two. First, you need to think about encoding or vectorizing your content, what you are ingesting it. Then at query time, when you use it, you want to vectorize the query as well, because in the end you are comparing vectors to vectors at that point. Once you have that done, the second part has to do with vector indexing. By that what I mean is you have a ton of content and you run this content through whatever preparation you need to, then through an encoder and put it in a vector database. Then when a query comes in, you take the query vector and you have to find the K-closest neighbors to that vector real quick. For large collections, doing this exhaustively as in comparing every vector is completely impractical, so in general, what we do is approximate nearest neighbor search, and there are a number of techniques to do that.

In the context of Azure, I'm very excited to announce that Azure Cognitive Search now has native support for vector search. We do this by introducing a new vector type that you can add to your indexes. Once you have that, you can use Azure Cognitive Search either as a pure vector search solution for the cases where you only want vectors, or as a hybrid retrieval solution where in many cases for these retrieval systems, you get the best quality of results by combining the best of traditional retrieval methods of keyword search based retrieval with vector based retrieval, where what we can do is we can query both fronts then fuse the results and present you the best candidate.

Additionally, we support combining this with our second step, or L2 re-ranking system that is powered by the same models that powers Bing. Such that by just enabling disk functionality, you can have us do hybrid retriever from vectors and from keywords, feed that into a secondary re-ranker where we'll re-rank for best results and then return that to the caller. All of that is built for you so you don't have to deal with the challenges of retrieval and you can focus on the functionality of the application. Of course, we'll do this in the context of Azure with enterprise-grade security and scalability and so on. Let's actually see this in action.

We can switch to the demo monitor. Excellent. Let me start here. What we'll do first is we'll just go and look at exactly what the API looks like. No fancy applications up top or anything, not even the SDK, we'll just do REST calls to see what does it look like to create and use a vector index. What I have here looks like a typical Azure search step to create an index where you define some fields. In this case, I have a searchable field, a filterable field. But I'm also introducing a vector field right here, and in this case I'm saying the dimensions. I'm using three so we can actually read this stuff. Practical numbers for these are usually in the hundreds, or maybe low thousands. Then indicating a vector configuration, which here it says what kind of vector index I want to create. In this case, I'm using HNSW. It's one of the ways you can organize vector indexes and I'm describing what is the metric; as in, how do we measure how close vectors are. In this case I'm saying cosine, we support a few other metrics like dot-product or Euclidean distance. Let me run this request.

Now I have a vector index that also, I have a hybrid index, it has vector and text. Next up, what I want to do is index some data. Again, this looks like a pretty normal Azure search, index, and request. It has three documents: first, second, and third. The only difference is one of the fields is a vector. Usually this is like maybe 1,500 floats. In this case I'm just having three so we can actually read through the stuff. I click send. Great. Now we have three documents indexed. Now let's run some vector queries.

The simplest query I can run is I can just say, hey, here's a vector and give me the closest 10 ones using as a reference this field number one. I'm going to run this guy. You can see I'm getting all three because in the end, they all match; the question is how far they are from the vector, and the score kind of indicates that. In fact, if I move this vector a little bit, and I'll put it in this position and run this query again, you can see that the first one is almost there. It's actually the same spot within precision. This was pure vector search. A couple of other interesting things you can do, one is, you can combine vectors with filters, maybe for security purposes or to customize the experience. In this case, I'm going to say filters and you can see these cats, category A, category B, and whatnot. What I'll do here is I'll say category equals B and run this query again. Now you can see I get the ranked results, but only the ones that fit the filter criteria before we evaluate the vector similarity.

Lastly, I talked about hybrid search. Very fancy sounding mechanics, but the way to use it is pretty simple. You just say search, and you give us keywords. In this case I'm going to add for the second document. I'm going to run this request, and you can see now, second document ranks higher because we're combining this course, we're fusing the retrieval from the keyword, some from the retrieval from the vectors, and then computing a few set of scores for them.

In this minute, what we do is we created a search index with a vector, indexed some vectors and then searched. Let's see this real quick on a complete application. This random battery thing I have open. All right, there. This is one of the example applications that we show, and actually this is in GitHub if anyone wants to try it.

Typically the way it works is we have an Azure search index, you give us a question, look at it in the index, and then feed it to the model to answer the question. Sometimes people can ask questions and there is always someone. They will ask a question like this: is the healthcare website for employees? They'll say, instead of just saying hearing device, they'll say, you know, some mechanical waves reaching the sides of my ear. The point is the retriever has no hope because, all these words don't quite mean anything from the retrieval standpoint, if you're looking for words. That's awesome. Let me do this. I promise this will work. Nothing like killing the thing and starting it over. There. One more second. I'm going to retry now, let's see if it's up. There. Now it's up. The bot has not much to work with because we didn't retrieve any document that is useful to it. Actually, I'm very glad that the model manages to say it doesn't know, because we don't want this model to spin. There is an answer somewhere in this knowledge base. The problem is the question is too contrived. Let me fix this by using Azure Cognitive Search Vectors. What I have here is, in my index, this is just the Azure portal. We can take a look at what's under the covers. You can see that I already created my content. I also have Azure embedding. I'm using the Azure OpenAI embeddings API for this.

Now I'm going to go to my code real quick here, this is where the search happens, and I'm going to add vector support here. What I'll do is I'll say query vector equals, and I need to embed the query as a vector. I'll say OpenAI, the embeddings, create. Then the engine I want to use, I have it in my configuration already. Then the input is q, is the query that I'm getting from the user. Once I have a vector, what I want to do is in my call to Azure Cognitive Search, I want to pass a vector as part of the query. I need three inputs. One is the actual value of the vector; in this case, is query vector which is calculated. K is how many I want to retrieve. I already have a top argument for that. Then fields is which fields to look into which the name of my field in this case is embedding. I actually need to dig into the response a little bit and get the actual part that I care about, which is the actual vector. We'll go with that. Let's put a breakpoint here and see what happens.

Now I'm going to ask the same question again. If I look at here what I hit the breakpoint, what I have here is the vector representation of this question. Actually, this works much better when you actually restart your application if you change the code so that you actually pick up your changes. Let's try one more time. There. Now I have an actual vector representation. This is what OpenAI computed as the encoded version of my question. Now I can send this to Azure Cognitive Search and then compute the results that then I'll send to the completions API to compute an answer. Now that we have actual data to work with, then the large language model can provide a reasonable answer to the question. It actually understood that I'm talking about a hearing aid device instead of some convoluted description of what it was. If we can go back to the slides.

Again, very excited to bring native vector search support as part of Azure Cognitive Search for all of you to create the next generation of large language models, powered applications on your own data. This is in private preview today. You can go to this link to sign up. To talk more about how they've been innovating using Azure OpenAI and Azure Cognitive Search, I want to roll a video from one of our customers, Schroders, innovating and trying in the real world some of this technology. Let's roll the video, please. 

JAMIE OVENDEN: I'm Jamie Ovenden. I'm the group CTO at Schroders. Schroders is a global investment manager. We manage nearly a trillion dollars of assets across private markets, public markets, and for wealth clients. Our entire investment performance relies on the understanding, the ingestion, the analytics analysis, and the output of data. The opportunities of generative AI in our space are absolutely huge.

CHARLOTTE WOOD: We think that generative AI has loads of exciting applications across our industry and others, and can also bring huge value to Schroders and to our clients. We can't wait to innovate with it. But we do need to balance that innovation with the need for security, data privacy, and client confidentiality, as well as making sure that we focus our resources on the right places. 

TIM GORDON: As someone who works with natural language processing, one of the things that's got me very excited about generative AI is reproducing applications which really do allow users to engage in activities and complete tasks that otherwise would have been impossible. Internally, we've deployed an AI system which we call Genie globally to our 6,000 users. It has both chat functionality and document QA functionality, which is supported by Azure OpenAI's implementation. The chat functionality is used for a wide range of tasks such as translation and also for coding tasks associated with the construction of systematic portfolios. Document Q and A allows users to interact with documents that are hundreds of pages long, far too long for them to read exhaustively, and allow them to ask questions that they otherwise might have missed. It uses a memory vector-based search to shortlist the sections of the document, which we then summarize using the large language models. We found the Azure OpenAI implementation to be really fast, really useful to have isolated resources between our environments, especially useful for low testing. Additionally, we found APIM to be an essential service during our efforts it really does come in useful for allowing us to track where costs are incurred around the various endpoints that we have in the application. 

JAMIE OVENDEN: Our next challenge is going to be scaling up. We've created huge demand within our population for the capabilities that we've delivered. Particularly, the ability to use internal documents in a secure way has really driven enthusiasm for the product. But the challenge we've now got is how do we create more holistic capability? How did we upload large amounts of data and document that Schroders has into the capability so that we can query it? Vector-based search is really the only option here. 

CHARLOTTE WOOD: We are looking forward to using the vector database capabilities within as Azure Cognitive Search to help us to evolve Genie further and make it more valuable for our users. 

TIM GORDON: When it comes to vector-based search, what we really need is our language developers to actually jump straight in and just rely on the service and have all the scalability concerns taken care of by that service. 

CHARLOTTE WOODS: There's so much opportunity with this that prioritization has been critical and finding the use cases that have a lot of value that are also unique to our organization. It's really important for us going forward so that we know where to focus our resource. 

PABLO CASTRO: Great. Thanks to our customer, Schroders. With that, I'll call Don back on stage. Thanks very much. 

DOM DIVAKARUNI: Thank you, Pablo. That was fantastic. It's pretty amazing to see how customers are leaning in with this technology faster, I think, than anyone has ever seen in other tech paradigms before, so it's pretty exciting and we're super happy to be able to help customers with that.

Another area that I want to share is plugins. We heard about plugins before. What are we doing in Azure OpenAI to help customers leverage plugins? Some of the problems we wanted to tackle with these models is where these models fall short today. They're great with lots and lots of different languages. Probably fall short a little bit in some tier two, tier three languages, and some Asian languages, African languages, etc. OpenAI also has something called the retrieval plugin that talks to multiple vector databases. We wanted to also incorporate that as a first-class thing with Azure OpenAI.

Lastly, we wanted to tie Azure OpenAI to information on the web so you can ground it with more relevant recent information as that's what's most useful, and build applications on top of all of this. And so happy to introduce Azure OpenAI Service Plugins, where this will be interoperable with all the plugins that OpenAI has, but what we're focused on first is to solve some of these key problems by integrating these Azure services or Microsoft services into our plugins to allow for these models to be extended for you to build applications on these models, extend them out to these various Azure services. All these sources are managed with Azure AD and managed identities, so your application can only access stuff that it's authorized to. Then additionally, because of some of the newness and the potential and learnings in the space, we want to ensure that we provide administrators with the control to only enable plugins that are approved in your enterprise for your Azure OpenAI resources.

You can administer what folks can see and what folks can't see. I'm going to quickly demo this and hopefully, the demo gods smile on me this time. At first, you don't succeed. In Azure OpenAI, I have a new section for plugins where I can enable the stuff. As you can see, I have three plugins enabled: Bing Search, Translator, and Cognitive Search, what Pablo was just demoing, and so I'm going to pretend here that we've built a Copilot that faces Contoso manufacturing corporation, and there's a problem with a part. There's a customer complaint that I'm trying to figure out what the customer is saying and help them resolve it. In this scenario, a customer e-mail came in in Vietnamese, and I don't know if I want to trust the model quality in trying to parse out the customer's concerns, so I want to use Azure Translator, which has really high translation accuracy. I go ahead and send that in and I want to make sure that it not only translates, but the model then acts on the translation to help summarize what are the client's most key concerns. We wait for this to come in.

Obviously, this is something that is in preview and that we're working on, and so what we're seeing here is that they're worried that proper due diligence hasn't been done on air quality. They want to make sure that the air quality standards in the documentation may be outdated, and so they want us to just double-check if everything is working right. Actually, if you look at this down here, we'll see that the Translator plugin has been used and we see the output of the actual translation. At this point, I'm going to ask it, hey, what are the testing standards? When I type in this, the model obviously says, I don't know this information. I should now reach out to Cognitive Search with my index, with my documents to understand what parts are relevant to extract that information out. Great, it says that these are the testing standards. It provided me with a number of tests, but it says that it's using a ISO standard, and I don't know if our testing standards have been up-to-date. This could be an old document. I'm going to go explore on the web to see what is the latest standard, and do I have that covered in my documentation? I'd say, when was ISO last updated? At this point, the plugin service reaches out to the Bing Search API and determines that the plugin was last updated in 2010, and if I go to the source it sure enough links to the ISO standard.

Well, this is going to load pretty slow, so I'm going to give up at that point. But as you can see, we've extended the model's capability to reach out to another external system. It'll help you build more powerful Copilots and leverage multiple sources of data that's out there. If I can go back to the slides now. Thanks much.

During the keynote, we covered configurable content filters. This is another important area of control that we wanted to introduce for our customers. Today, we run content filtering in line with the service. We filter out the prompts and completions to ensure that no harmful content in these following categories are filtered. What we've now enabled is the ability to adjust the sensitivity of your content filters. By default, medium-severity harms are filtered out. You can adjust that now to say, hey, you know what, in the sexual category I don't want low-severity harms as well, so you can filter that out. There's plenty of great content. It looks like we ran a little long with some demo issues, so I don't want to hold you all up here. The last thing I'd say is that we have Provision Throughput, which is an offering for production applications running at really large-scale to ensure that they can reserve the capacity they need for their production applications.

It's going to be available in very limited form for large-scale applications on our service. Please reach out to your account team if you want to learn more. That's it. I'll recap with some next steps. We covered a lot of announcements here. Please take a picture of that QR code, visit it and you'll get updates on when the stuff is available. All of these are coming early June, so please tune in for these updates and we look forward to seeing what you'll build with it. Thank you very much.

END
