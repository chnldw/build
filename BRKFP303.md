Microsoft Build
How to Build Next-Gen AI Services with NVIDIA AI on Azure Cloud
Adel El Hallak, Jon Coons, Damian Hasak
May 2023

ADEL EL HALLAK: Hello everyone, my name is Adel El Hallak. I'm the Senior Director of Enterprise Software Product Management for NVIDIA AI. I'm super excited to be presenting on Microsoft Build 2023 with my esteemed partners in crime: Jon Coons, Senior Global Black Belt for Azure AI, and Damian Hasak, Director of Azure HPC and AI Solutions. In today's session, we'll share trends we're seeing AI, and then quickly pivot to talk through the NVIDIA AI platform and how we're partnering with Microsoft to make NVIDIA AI software accessible across an array of different Azure services, and ultimately, meeting developers where they are. 

We'll wrap up with a demo to show how all this works in Azure ML, and save time at the end for some Q&A. In the past, we've said that every industry will leverage AI, but today's reality is that every business function can be infused with AI. IDC predicts that the world's 2,000 largest companies will use AI across every critical business function. To highlight a few, IT and information security use AI to identify cybersecurity threats in real-time and quickly address them before they proliferate. 

Operations will use AI for predictive maintenance, so they can service machines before they break, and they’re demand modeling, so they are not over or short on inventory. In product development, we use AI to do digital prototyping and design to get from development to production quicker and more cost-effectively. The point is, AI will be infused in every industry and across every critical business function, and enterprise will need to adapt to remain competitive in their respective markets. 

This is why you see a turning point for AI in the Cloud, and enterprises have challenges deploying AI, with slightly over half of the projects moving to production and taking over seven months to go from pilot to actual implementation. These challenges can be a result of lack of in-house expertise, lack of compute resources or data, and complexity of scaling the points. The Cloud, and the Azure Cloud particularly, helps overcome some of these barriers. The reliance of enterprises on Cloud computing continues to grow, with at least half of computing intensive AI workloads running on the Cloud. 

Developers in organizations are looking to NVIDIA and Microsoft to help them through the challenges and get to production quicker, more efficiently, while lowering total cost of ownership. NVIDIA-pioneered accelerated computing can tackle challenges no one else can solve. We have been advancing the art of AI for over a decade, and have created a rich ecosystem built on the NVIDIA AI platform, which includes over 450 SDKs and pre-trained models used by over four million developers across 40,000 companies. 

Built on the foundation of NVIDIA GPU and CUDA infrastructure, the NVIDIA AI platform accelerates the entire AI pipeline, from data processing to training and inference. How are we ushering in this new wave of AI? It is through a full- stack optimization. Our accelerators, available through instances in the Azure Cloud, deliver the highest throughput for all sorts of different workloads. That's proven by multiple ways in peer-reviewed, industry-standard benchmarks, such as MLPerf. As I alluded earlier, it's not just the hardware accelerators that are delivering performance. 

NVIDIA is a full-stack company and we optimize across our drivers, all the CUDA-X libraries, third-party components, and open-source software, such as PyTorch, Thrust, Spark, RAPIDS, and much more. Equally important, we have a unified acceleration architecture that allows us to provide a wide range of SDKs and workflows, addressing all sorts of use cases and across all business functions. I'll just get into the details of NVIDIA AI, and how we are helping developers and enterprises realize value. NVIDIA AI is a full-stack solution for building and deploying accelerated AI in production. 

We provide a range of AI frameworks across a broad set of use cases and industries, to help organizations accelerate time to value. Examples include Metropolis, our video analytics framework, and one that will be used in our demo session later. We also have Morpheus for detecting real-time threats, RIVA for speech AI, Merlin for building recommender systems, cuOpt for route optimization, and MONAI for medical imaging, to name a few. Our commercial offering is NVIDIA AI Enterprise, which not only provides access to these frameworks, but is also augmented with exclusive Enterprise capabilities and features. 

Let me tell you a little more about the benefits of being on NVIDIA AI Enterprise Subscription. The first benefit of NVIDIA AI Enterprise is increased throughput, which in turn yields improvements in productivity and a lower TCO. You can essentially do more with less. We also provide reference workflows that utilize our AI frameworks and pre-trained models to provide examples for specific AI workups, long enterprises to rapidly prototype. NVIDIA AI Enterprise also comes with Enterprise support and a defined SLA, while providing security and API stability for our production branches. 

Finally, perspective of where you choose to build and ultimately deploy, we have your backs covered as we optimize and certify our stats, from Cloud, to on-prem and the Edge. Let’s look a little closer at each benefit. At a high level, developing AI use cases follows the same pattern. Data prep, model training, and ultimately, inference. It starts with data processing, and with NVIDIA Rapids, data science processes can be reduced from hours to seconds. When using NVIDIA A100 GPU, five times faster performance can be seen compared to similar CPU-only configurations, resulting in a four times reduction in cost, with a reduced power consumption and carbon footprint. 

Our pre-trained models with the likes of computer vision, speech, and more, provide developers or data scientists with the starting points, so they are not training from scratch. Using the tab toolkit, they can fine-tune the models with a custom build in hours, rather than a month. Optimize for Inference service is an open-source entrance starting software that helps standardized model deployment and execution and delivers fast and scalable AI in production. When used with the GPU like NVIDIA A100, 146 times performance increase can be seen when compared to running the same workload, on the same server, with just a CPU. 

Ultimately, NVIDIA AI Enterprises is a unified platform for accelerated workloads and increases throughput in productivity while reducing costs and energy consumption at every step of your AI journey. We deliver NVIDIA AI Enterprise software to containers. Containers encapsulate an application with all its dependencies. This enables portability, but equally important, it ensures data scientists and developers aren't wasting precious time figuring out what the optimal set of dependencies are in very fast-moving open-source world. 

The illustration on the left shows the complexity of dependencies of both third-party and NVIDIA components in optimizing TensorRT. We're talking about nearly 30 NVIDIA and more than 400 third-party components for our inference optimizer and runtime alone. Point is, we take care of that plumbing, so you can just focus on your use case. We do these optimizations repeatedly, often on a monthly case. As we continue evolving AI software, without any changes to the hardware, the advances in NVIDIA AI are able to deliver up the 54 percent more performance in six months on the same hardware infrastructure. 

As the left of this slide demonstrates, the work it takes to optimize our containers, the graph on the right shows why we do this. You get speedups on the same hardware through software optimizations alone. While performance is important, it's equally important that we recognize where enterprises are, and their deployment journeys. What I described earlier is what I call a "roll-forward branch." We optimize the latest version of the framework, we patch for critical and high-common vulnerabilities and exposures, or CVEs, and publish on monthly cadence. 

This gives developers access to the latest features and performance optimizations. There are times when enterprises want to standardize on an API and not introduce any breaking changes. Inference deployments are typical workflows for such needs. For this, we have our production branches, which we're making available in the second half of this year. This is two branches per year, each having a nine-month lifeline. For a given production branch, we patch critical and high CVEs on a monthly basis, while ensuring we maintain API stability from one month to the next. 

Our branches also have a three-month overlap period, giving developers a time to transition, in case there are API changes for migrating from one production to the next. Finally, for highly regulated industries such as medical, telco, and auto, we have long- term supported branches. We maintaining API stability for 36 months, while patching CVEs and bug fixes on quarterly basis, and giving enterprises six months to roll from one long-term supported branch to another. 

Through the NVIDIA AI Enterprise subscription, enterprises have access to all three branches, providing them with peace of mind, knowing they have the flexibility to choose a branch that is best for their development and deployment without compromising on security. Another set of features, exclusive to NVIDIA AI Enterprise, are AI workflows. AI workflows are Cloud-native, prepackaged reference examples that will illustrate how NVIDIA AI frameworks can be leveraged to deliver AI outcomes. 

Through a combination of training and inference pipelines, delivered as super notebooks and help trots, we provide patterns for creating intelligent virtual assistance, transcription in multiple languages, detection of threats across digital fingerprints, personalized recommendations, and more. We're increasingly growing the set of ramps applications, one for route optimizations, and inference and scale are coming soon, so make sure to check out the AI Workflows page for the latest. 

Ultimately, these AI workflows help organizations accelerate their path to AI outcomes. Bringing all this together, the AI Enterprise is the unified AI platform for accelerated workloads. We help you improve productivity through performance-optimized containers, the AI workflows, or pre-trained models. We provide you with access to different branches, whether you're looking for the latest features or looking to secure API reliability and long-term supported branches. We offer them to you simultaneously through one subscription. With Enterprise support from NVIDIA, you get to tap into technical experts with the finest LA’s for responses, knowing you have the backing of NVIDIA as you infuse your organization with AI. 

Today, I'm pleased to announce that all the goodness we just discussed is now available on the Azure Marketplace, giving enterprises who have committed Cloud spend agreements with Azure the ability to break down their credits. NVIDIA AI Enterprise comes in a couple of options. The on-demand listing starts at a dollar per hour, plus Enterprise support is included in this model, but will be limited to three calls total. If users need or would like additional support, they can contact NVIDIA for a private offer, which is option 2. 

A private offer is a custom quote that's negotiated with NVIDIA for longer-term commitments at a discounted rate. The majority of customers who are using NVIDIA AI Enterprise will likely be interested in the private offers. A bring your own license model is also available, in case you've already purchased a license, or are looking to source from a channeled partner. NVIDIA publishes the GPU-optimized VM live on the Azure Marketplace, should users be looking for a virtual machine and bring your own license model. Super excited for this, now live on the Azure Marketplace, but we're not stopping here. Now I'm going to turn it over to Jon to talk to you a little more about how NVIDIA is further partnering with Microsoft for AI in the Azure Cloud. 

JON COONS: Thanks, Adel. Jon Coons with the AI Global Black Belt Team at Microsoft, focused solely on our machine learning and AI products within Azure. I want to look at the Azure Machine Learning Studio, a little bit more, the high-level flow, how we derive insight from structured and unstructured data, turn that into value, or on the right-hand side, with the business apps and the analytics. 

If you're a no-code type of a developer or business analyst, or if you want to look at low code, if you know something of data scientists, or if you're pro-code. What we offer in Azure Machine Learning is the most comprehensive, end-to-end machine learning platform available, powering both data scientists, developers, citizen data scientists, across a wide range of productive experiences for building, training, and deploying models. When we couple that with what NVIDIA AI Enterprise is bringing to the table, they're going to be exposing that in the Azure ML Registry, that's a new feature that we brought out to be able to share assets across different work zones within Azure. 

If you bring that into your workspace, you can leverage those pipelines and automation to expedite your workflow across, whether you're looking at rapids for accelerating, machine learning traditional models, or you're looking at data exploration and want to use something like qSpark or QDF. It's all going to be inside of that registry. Whether you're looking at a dev and test, or you're in QA and prod, or moving that through the pipeline within your MLOps, that tooling is going to be available to you if you want to look at how we leverage the model validation, the AI evaluation. 

How we look at responsible AI, being able to apply that to those models, and create the best combination of both worlds. The value becomes accentuated with NVIDIA AI Enterprise. When we jump over to looking at optimized experience inside of Azure Machine Learning, looking at the infrastructure or bringing to the table our best-in-class, high-performance compute Gen 2 VMs, along with HPC-based images within Ubuntu and others. Being able to leverage that across with the curated environments that NVIDIA AI Enterprise is going to bring to the table. 

Being able to offload all of that security and compliance that you need to pay attention to, how the CV's and make them up, or breaking changes that happen within the containers, being able to offload that back to NVIDIA with long-term releases of their acceleration software, the CUDA software, running on those GPUs. Just going to expedite everything that you do from the top down, so that speed, the security, the simplicity, all come together to drive NVIDIA AI Enterprise and Azure Machine Learning to help you achieve more with what you're trying to accomplish. Now, over to Damian for how we're leveraging the infrastructure within Azure to power the NVIDIA acceleration. 

DAMIAN HASAK: Thanks, Jon. Damian Hasak, Director for Azure HPC and AI Solutions in Azure. AI at scale is really built into Azure's DNA, from our initial investments and large language model research, Turing, and engineering milestones, such as building the first AI supercomputer in the Cloud, really prepared us for the moment when generative AI became possible. Azure provides a range of GPU-optimized tools and services, including Azure Machine Learning, Azure Cognitive Services, and Azure Kubernetes Services, just to name a few, and these services enable businesses to build and deploy AI and machine learning applications quickly and efficiently using tools and frameworks such as TensorFlow and PyTorch. 

Overall, our partnership with NVIDIA has allowed us to offer powerful GPU-enabled infrastructure for businesses of all sizes take advantage of AI and machine learning at scale. As I said, scale's always been a part of our North Star at Azure, to optimize for AI. We're now bringing super computing capabilities to startups and companies of all sizes, without requiring the capital for massive hardware and software investments. The Azure N-series Virtual Machines are designed specifically for GPU-intensive workloads, like machine learning, deep learning, and high-performance computing. 

These machines are powered by NVIDIA GPUs and offer a range of options, including V100, T4, A100, A10, which has support for GPU partitioning, and now the new H100, which are available on preview today. With InfiniBand networking, on the Azure N-series VMs, communication between GPUs is significantly faster, which can improve the performance of machine learning and other GPU-intensive applications. InfiniBand networking allows Azure VMs to communicate with each other with very low latency, high bandwidth, and low CPU overhead. 

This also enables high-performance parallel computing for distributed machine learning workloads where data can be distributed across multiple VMs and GPUs, resulting in faster training times and improved performance. NVIDIA and Microsoft have worked together for years to bring the latest AI innovations to businesses all over the globe. With the H100 virtual machines, we're taking things to the next level, ushering in a new era of generative AI applications and services that will drive innovation across a range of industries. 

H100 has powerful features, like 400 gigabit a second NVIDIA Quantum, to InfiniBand connections, NVSwitch, and NVLink 4.0 with over 3.6 terabytes a second bisectional bandwidth between local GPUs within each VM. This allows for massive scale. Building on top of this, NVIDIA AI Enterprise brings a clear range of benefits to companies running on Azure who are looking to take advantage of AI and machine learning at scale. 

So, by partnering with NVIDIA, we've further optimized our infrastructure to provide even greater value to our customers. With our extensive experience in delivering multi exaflop supercomputers to Azure customers worldwide, we've developed infrastructure that provides true supercomputer performance. Our partnership with NVIDIA has allowed us to deliver the best possible experience on Azure through NVIDIA AI Enterprise, offering curated LTS environments and so much more, enhancing the VM images available on Azure. 

This collaboration enables businesses to confidently rely on our combined solution to provide the power required for their AI and machine learning applications. By using Azure and NVIDIA AI Enterprise, companies can take advantage of a range of benefits, including faster training times, improved performance, and better accuracy. Over to you, Adel.  

ADEL EL HALLAK: Thanks, Damian. Rather than just take you through slides, we're going to show you an example of how easy it is to build a Computer Vision Workflow with NVIDIA AI Enterprise and Azure ML. We're going to create an inference pipeline, test and model, fine-tune and optimize the model to improve its accuracy, then redeploy it to see if it improved. Pose estimation is used frequently in Computer Vision Workflows involving people; in retail, in autonomous shopping, it is leveraged to recognize actions such as a person picking up an item on the shelf. 

In healthcare, it can be used for patient monitoring or fall detection. In sports and entertainment, it can be used to analyze athlete actions, such as someone shooting a basketball or someone doing some horrible dancing. In manufacturing, it can help in worker safety and identifying dangerous behavior on the factory floor. Point is, pose estimation can be used across an array of different use cases and across industries. To help you get started, we've played assets specific to pose estimation inside the NVIDIA AI Enterprise tech preview registry in Azure ML for you to try it out. 

We have a pre-trained model for pose estimation, the TAO Toolkit for fine-tuning computer vision models, and the DeepStream SDK for real-time video analytics inference. The registry also contains environments, which are made up of NVIDIA AI Enterprise software that I've just referenced, along with the corresponding pre-trained models, but also has command components to build pipelines for fine-tuning the model and custom data, and then deploying in production for inference. Using the published environment and components, we can quickly create a body pose estimation inference pipeline through the sample app provided. 

All you have to do is modify the input file locations for the original video to the inference time. Here is the annotated output video after inference. This is me making a fool of myself attempting to dance. You can see some bounding boxes but clearly, the model needs to be improved, since it is often not recognizing my lower torso and legs. To improve accuracy, we need additional data with which we can fine-tune this model. We use the key point detection data for pose estimation from the COCO data set. You can obviously also leverage your own data, because we built the pipeline component that can be easily modified to point and train on custom data. 

Now I'm going to use this ready-to-use pipeline component to run the fine-tuning process. It acts as a visual programming language for building AI workflows. You can create and modify the command components, where they are represented as a visual diagram that links together sequences of multiple components. The output of one process is fed as input to the next process. You can run this pipeline to validate your new optimized model's improved accuracy, inspect the inference results of your Azure inference. Now let's see the output of the improved model. 

Now it looks much better, and I'm not talking about my dancing, but rather the inference model. It detects all the body pose key points, including my legs. Once you are happy with the results, you can leverage the endpoints and deploy your new fine-tuned and optimize model for real-time inference. In addition to pose estimation example, we also have other NVIDIA AI Enterprise samples for computer vision, ones for medical imaging in healthcare, RAPIDS examples for processing tabular data in machine learning, and examples for doing inference with Triton. 

All these are already pre-populated in the NVIDIA AI Enterprise Tech Preview Registry. To summarize, developing intelligent AI models for such use cases is time- and resource-intensive. NVIDIA AI Enterprise software increases your productivity and reduces time to market by providing curated samples in the Azure ML NVIDIA AI Enterprise Tech Preview Registry. Go ahead and leverage the pre-built pipelines, factor models with your custom data, and deploy your optimized AI models at scale. 

Check out the NVIDIA AI Enterprise Tech Preview Registry in Azure ML for the examples I just talked about. We also have NVIDIA AI Enterprise available in the Azure Marketplace today. Please make sure to tune in for the NVIDIA AI Enterprise Registry for Azure ML Deployments of AI Workflows on May 24th at 12:15 for more details. And finally, make sure to check out our visual virtual booth, where you can get an opportunity to enroll in our developer program, among other things. Thank you very much. 

END
